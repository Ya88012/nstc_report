@article{chemla2013bike,
    title={Bike sharing systems: Solving the static rebalancing problem},
    author={Chemla, Daniel and Meunier, Fr{\'e}d{\'e}ric and Calvo, Roberto Wolfler},
    journal={Discrete Optimization},
    volume={10},
    number={2},
    pages={120--146},
    year={2013},
    publisher={Elsevier}
}

@ARTICLE{8114708,
    author={Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
    journal={Proceedings of the IEEE},
    title={Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
    year={2017},
    volume={105},
    number={12},
    pages={2295-2329}
}

@ARTICLE{9093851,
    author={Lu, Eric Hsueh-Chan and Lin, Zhan-Qing},
    journal={IEEE Access}, 
    title={Rental Prediction in Bicycle-Sharing System Using Recurrent Neural Network}, 
    year={2020},
    volume={8},
    number={},
    pages={{92262}-{92274}},
    doi={10.1109/ACCESS.2020.2994588}
}

@article{simonyan2014very,
    title={Very deep convolutional networks for large-scale image recognition},
    author={Simonyan, Karen and Zisserman, Andrew},
    journal={arXiv preprint arXiv:1409.1556},
    year={2014}
}

@inproceedings{he2016deep,
    title={Deep residual learning for image recognition},
    author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={770--778},
    year={2016}
}

@inproceedings{10.5555/2986459.2986743,
    author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
    title = {Algorithms for Hyper-Parameter Optimization},
    year = {2011},
    booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
    pages = {2546–2554},
    numpages = {9},
    series = {NIPS'11}
}

@article{kotsiantis2006data,
    title={Data preprocessing for supervised leaning},
    author={Kotsiantis, Sotiris B and Kanellopoulos, Dimitris and Pintelas, Panagiotis E},
    journal={International journal of computer science},
    volume={1},
    number={2},
    pages={111--117},
    year={2006}
}

@article{famili1997data,
    title={Data preprocessing and intelligent data analysis},
    author={Famili, A and Shen, Wei-Min and Weber, Richard and Simoudis, Evangelos},
    journal={Intelligent data analysis},
    volume={1},
    number={1},
    pages={3--23},
    year={1997}
}

@article{WARING2020101822,
    title = {Automated machine learning: Review of the state-of-the-art and opportunities for healthcare},
    journal = {Artificial Intelligence in Medicine},
    volume = {104},
    pages = {101822},
    year = {2020},
    issn = {0933-3657},
    author = {Jonathan Waring and Charlotta Lindvall and Renato Umeton}
}

@article{elsken2019neural,
    title={Neural architecture search: A survey},
    author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
    journal={The Journal of Machine Learning Research},
    volume={20},
    number={1},
    pages={1997--2017},
    year={2019},
    publisher={JMLR. org}
}

@article{zhao2020simplifying,
    title={Simplifying architecture search for graph neural network},
    author={Zhao, Huan and Wei, Lanning and Yao, Quanming},
    journal={arXiv preprint arXiv:2008.11652},
    year={2020}
}

@InProceedings{pmlr-v80-bender18a,
    title =   {Understanding and Simplifying One-Shot Architecture Search},
    author =       {Bender, Gabriel and Kindermans, Pieter-Jan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc},
    booktitle =   {Proceedings of the 35th International Conference on Machine Learning},
    pages =   {550--559},
    year =   {2018},
    editor =   {Dy, Jennifer and Krause, Andreas},
    volume =   {80},
    series =   {Proceedings of Machine Learning Research},
    publisher =    {PMLR}
}

@article{NARMADHA2021,
    title = {Spatio-Temporal vehicle traffic flow prediction using multivariate CNN and LSTM model},
    journal = {Materials Today: Proceedings},
    year = {2021},
    author = {S. Narmadha and V. Vijayakumar}
}

@INPROCEEDINGS{8171119,
    author={Liu, Yipeng and Zheng, Haifeng and Feng, Xinxin and Chen, Zhonghui},
    booktitle={2017 9th International Conference on Wireless Communications and Signal Processing (WCSP)},
    title={Short-term traffic flow prediction with Conv-LSTM},
    year={2017},
    pages={1-6}
}

@INPROCEEDINGS{8916852,
    author={Mihaita, Adriana-Simona and Li, Haowen and He, Zongyang and Rizoiu, Marian-Andrei},
    booktitle={2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
    title={Motorway Traffic Flow Prediction using Advanced Deep Learning},
    year={2019},
    volume={},
    number={},
    pages={1683-1690},
}

@article{PARK2017154,
    title = {An optimization approach for the placement of bicycle-sharing stations to reduce short car trips: An application to the city of Seoul},
    journal = {Transportation Research Part A: Policy and Practice},
    volume = {105},
    pages = {154-166},
    year = {2017},
    issn = {0965-8564},
    author = {Chung Park and So Young Sohn}
}

@INPROCEEDINGS{8621918,
    author={Zhou, Yang and Huang, Yan},
    booktitle={2018 IEEE International Conference on Big Data (Big Data)},
    title={Context Aware Flow Prediction of Bike Sharing Systems},
    year={2018},
    volume={},
    number={},
    pages={2393-2402},
    doi={10.1109/BigData.2018.8621918}
}

@article{YANG2020101521,
    title = {Using graph structural information about flows to enhance short-term demand prediction in bike-sharing systems},
    journal = {Computers, Environment and Urban Systems},
    volume = {83},
    pages = {101521},
    year = {2020},
    issn = {0198-9715},
    author = {Yuanxuan Yang and Alison Heppenstall and Andy Turner and Alexis Comber}
}

@article{Lv2015TrafficFP,
    title={Traffic Flow Prediction With Big Data: A Deep Learning Approach},
    author={Yisheng Lv and Yanjie Duan and Wenwen Kang and Zheng Xi Li and Feiyue Wang},
    journal={IEEE Transactions on Intelligent Transportation Systems},
    year={2015},
    volume={16},
    pages={865-873}
}

@article{shi2015convolutional,
    title={Convolutional {LSTM} network: {A} machine learning approach for precipitation nowcasting},
    author={Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-chun},
    journal={Advances in neural information processing systems},
    volume={28},
    year={2015}
}

@inproceedings{yao2019revisiting,
    title={Revisiting spatial-temporal similarity: A deep learning framework for traffic prediction},
    author={Yao, Huaxiu and Tang, Xianfeng and Wei, Hua and Zheng, Guanjie and Li, Zhenhui},
    booktitle={Proceedings of the AAAI conference on artificial intelligence},
    volume={33},
    number={01},
    pages={5668--5675},
    year={2019}
}

@article{zoph2016neural,
    title={Neural architecture search with reinforcement learning},
    author={Zoph, Barret and Le, Quoc V},
    journal={arXiv preprint arXiv:1611.01578},
    year={2016}
}

@INPROCEEDINGS{8477735,
    author={Wang, Bin and Sun, Yanan and Xue, Bing and Zhang, Mengjie},
    booktitle={2018 IEEE Congress on Evolutionary Computation (CEC)},
    title={Evolving Deep Convolutional Neural Networks by Variable-Length Particle Swarm Optimization for Image Classification},
    year={2018},
    volume={},
    number={},
    pages={1-8}
}

@ARTICLE{8712430,
    author={Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G.},
    journal={IEEE Transactions on Evolutionary Computation},
    title={Evolving Deep Convolutional Neural Networks for Image Classification},
    year={2020},
    volume={24},
    number={2},
    pages={394-407}
}

@article{brock2017smash,
    title={Smash: one-shot model architecture search through hypernetworks},
    author={Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
    journal={arXiv preprint arXiv:1708.05344},
    year={2017}
}

@inproceedings{pedregosa2016hyperparameter,
    title={Hyperparameter optimization with approximate gradient},
    author={Pedregosa, Fabian},
    booktitle={Proceedings of the International conference on machine learning},
    pages={737--746},
    year={2016},
}

@article{liu2018darts,
    title={{DARTS}: Differentiable architecture search},
    author={Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
    journal={arXiv preprint arXiv:1806.09055},
    year={2018}
}

@inproceedings{guo2020single,
    title={Single path one-shot neural architecture search with uniform sampling},
    author={Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
    booktitle={Proceedings of the European Conference on Computer Vision},
    pages={544--560},
    year={2020},
}

@inproceedings{real2019regularized,
    title={Regularized evolution for image classifier architecture search},
    author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
    booktitle={Proceedings of the aaai conference on artificial intelligence},
    volume={33},
    number={01},
    pages={4780--4789},
    year={2019}
}

@InProceedings{pmlr-v64-guyon_review_2016,
    title = {A brief Review of the ChaLearn AutoML Challenge: Any-time Any-dataset Learning without Human Intervention},
    author = {Guyon, Isabelle and Chaabane, Imad and Escalante, Hugo Jair and Escalera, Sergio and Jajetic, Damir and Lloyd, James Robert and Macià, Núria and Ray, Bisakha and Romaszko, Lukasz and Sebag, Michèle and Statnikov, Alexander and Treguer, Sébastien and Viegas, Evelyne},
    booktitle = {Proceedings of the Workshop on Automatic Machine Learning},
    pages = {21--30},
    year = {2016},
    editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
    volume = {64},
    series = {Proceedings of Machine Learning Research},
    address = {New York, New York, USA},
    month = {24 Jun},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v64/guyon_review_2016.pdf},
    url = {https://proceedings.mlr.press/v64/guyon_review_2016.html},
    abstract = {The ChaLearn AutoML Challenge team conducted a large scale evaluation of fully automatic, black-box learning machines for feature-based classification and regression problems. The test bed was composed of 30 data sets from a wide variety of application domains and ranging across different types of complexity. Over five rounds, participants succeeded in delivering AutoML software capable of being trained and tested without human intervention. Although improvements can still be made to close the gap between human-tweaked and AutoML models, this challenge has been a leap forward in the field and its platform will remain available for post-challenge submissions at http://codalab.org/AutoML.}
}

@InProceedings{pmlr-v119-real20a,
    title = {{A}uto{ML}-Zero: Evolving Machine Learning Algorithms From Scratch},
    author = {Real, Esteban and Liang, Chen and So, David and Le, Quoc},
    booktitle = {Proceedings of the 37th International Conference on Machine Learning},
    pages = {8007--8019},
    year = {2020},
    editor = {III, Hal Daumé and Singh, Aarti},
    volume = {119},
    series = {Proceedings of Machine Learning Research},
    month = {13--18 Jul},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v119/real20a/real20a.pdf},
    url = {https://proceedings.mlr.press/v119/real20a.html},
    abstract = {Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks—or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.}
}

@inproceedings{10.1145/3321707.3321721,
    author = {Liang, Jason and Meyerson, Elliot and Hodjat, Babak and Fink, Dan and Mutch, Karl and Miikkulainen, Risto},
    title = {Evolutionary Neural AutoML for Deep Learning},
    year = {2019},
    isbn = {9781450361118},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3321707.3321721},
    doi = {10.1145/3321707.3321721},
    abstract = {Deep neural networks (DNNs) have produced state-of-the-art results in many benchmarks and problem domains. However, the success of DNNs depends on the proper configuration of its architecture and hyperparameters. Such a configuration is difficult and as a result, DNNs are often not used to their full potential. In addition, DNNs in commercial applications often need to satisfy real-world design constraints such as size or number of parameters. To make configuration easier, automatic machine learning (AutoML) systems for deep learning have been developed, focusing mostly on optimization of hyperparameters.This paper takes AutoML a step further. It introduces an evolutionary AutoML framework called LEAF that not only optimizes hyperparameters but also network architectures and the size of the network. LEAF makes use of both state-of-the-art evolutionary algorithms (EAs) and distributed computing frameworks. Experimental results on medical image classification and natural language analysis show that the framework can be used to achieve state-of-the-art performance. In particular, LEAF demonstrates that architecture optimization provides a significant boost over hyperparameter optimization, and that networks can be minimized at the same time with little drop in performance. LEAF therefore forms a foundation for democratizing and improving AI, as well as making AI practical in future applications.},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
    pages = {401–409},
    numpages = {9},
    keywords = {neural networks/deep learning, artificial intelligence, AutoML},
    location = {Prague, Czech Republic},
    series = {GECCO '19}
}

@ARTICLE{6791438,
    author={Bäck, Thomas and Schwefel, Hans-Paul},
    journal={Evolutionary Computation}, 
    title={An Overview of Evolutionary Algorithms for Parameter Optimization}, 
    year={1993},
    volume={1},
    number={1},
    pages={1-23},
    doi={10.1162/evco.1993.1.1.1}
}

@inproceedings{li2015traffic,
    title={Traffic prediction in a bike-sharing system},
    author={Li, Yexin and Zheng, Yu and Zhang, Huichu and Chen, Lei},
    booktitle={Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems},
    pages={1--10},
    year={2015}
}

@inproceedings{zhang2017deep,
    title={Deep spatio-temporal residual networks for citywide crowd flows prediction},
    author={Zhang, Junbo and Zheng, Yu and Qi, Dekang},
    booktitle={Proceedings of the Thirty-first AAAI conference on artificial intelligence},
    year={2017},
    pages={1655--1661}
}

@article{10.1162/neco.1997.9.8.1735,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
}

@inproceedings{SAGAON,
    title={An Effective Evolutionary Neural Architecture Search for Bike-Sharing System Demand Prediction},
    author={Bo-Han Chen and Yun-Ye Cai and Chao-Yen Huang and Chun-Wei Tsai},
    booktitle={Proceedings of the IEEE Applied Sensing Conference},
    year={2023}
}
